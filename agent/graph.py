"""Agent de recommandation de voyages - Examen RNCP37805BC03

Architecture minimaliste : 1 n≈ìud unique
Structured output : extraction de 6 crit√®res bool√©ens
Catalogue : 5 voyages pr√©d√©finis
RESET obligatoire des crit√®res √† chaque tour

INT√âGRATION LANGSMITH pour monitoring et tra√ßabilit√©
"""

from __future__ import annotations
import os
from dotenv import load_dotenv
from dataclasses import dataclass, field
from typing import Any, Dict, Optional

from pydantic import BaseModel, Field
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, END
from langsmith import traceable
# =============================
#   CHARGEMENT .ENV (NOUVEAU)
# =============================



# Charger le fichier .env s'il existe
# Par d√©faut, cherche .env dans le r√©pertoire courant
load_dotenv()

# =============================
#   CONFIGURATION LANGSMITH
# =============================

# Variables d'environnement LangSmith (√† d√©finir dans .env)
# LANGSMITH_API_KEY=votre_cl√©_api
# LANGSMITH_PROJECT=voyage-agent-examen
# LANGSMITH_TRACING=true

# Lecture des variables LangSmith depuis .env
LANGSMITH_TRACING = os.getenv("LANGSMITH_TRACING", "false")
LANGSMITH_PROJECT = os.getenv("LANGSMITH_PROJECT", "voyage-agent-examen")
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")

# Affichage de l'√©tat de LangSmith (lecture depuis .env uniquement)
if LANGSMITH_TRACING.lower() == "true":
    LANGSMITH_ENABLED = True
    print("‚úÖ LangSmith activ√© depuis .env - Tra√ßage des op√©rations")
    print(f"   Projet: {LANGSMITH_PROJECT}")
    
    if not LANGSMITH_API_KEY:
        print("‚ö†Ô∏è  ATTENTION: LANGSMITH_API_KEY non d√©finie dans .env")
        print("   Le tra√ßage ne fonctionnera pas sans cl√© API")
else:
    LANGSMITH_ENABLED = False
    print("‚ö†Ô∏è  LangSmith d√©sactiv√© - D√©finir LANGSMITH_TRACING=true dans .env")


# =============================
#   CONFIGURATION MISTRAL AI
# =============================

# Variable d'environnement requise pour Mistral AI (√† d√©finir dans .env)
# MISTRAL_API_KEY=votre_cl√©_api_mistral

# V√©rification de la cl√© API Mistral
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

if not MISTRAL_API_KEY:
    print("‚ùå ERREUR: MISTRAL_API_KEY non d√©finie")
    print("   Ajoutez MISTRAL_API_KEY=votre_cl√© dans le fichier .env")
    raise ValueError("MISTRAL_API_KEY est requis pour utiliser le mod√®le Mistral AI")
else:
    print("‚úÖ Cl√© API Mistral configur√©e")


# =============================
#   PYDANTIC SCHEMA (Structured Output)
# =============================

class Criteres(BaseModel):
    """Sch√©ma pour l'extraction structur√©e des crit√®res"""
    plage: Optional[bool] = Field(None, description="Vacances √† la plage, mer, oc√©an")
    montagne: Optional[bool] = Field(None, description="Vacances en montagne, ski, altitude")
    ville: Optional[bool] = Field(None, description="Vacances en ville, urbain, m√©tropole")
    sport: Optional[bool] = Field(None, description="Activit√©s sportives, randonn√©e")
    detente: Optional[bool] = Field(None, description="S√©jour d√©tente, repos, spa")
    acces_handicap: Optional[bool] = Field(None, description="Accessibilit√© PMR, handicap")


# =============================
#   STATE (conforme examen)
# =============================

@dataclass
class State:
    """√âtat de l'agent - Stocke uniquement le dernier √©change"""
    dernier_message_utilisateur: str = ""
    dernier_message_ia: str = ""
    criteres: Dict[str, Optional[bool]] = field(
        default_factory=lambda: {
            "plage": None,
            "montagne": None,
            "ville": None,
            "sport": None,
            "detente": None,
            "acces_handicap": None
        }
    )


# =============================
#   CATALOGUE VOYAGES
# =============================
# Ordre optimis√© : voyages "premium" / sp√©cifiques en priorit√©

VOYAGES = [
    {
        "nom": "5 √©toiles √† Chamonix option ski",
        "labels": ["montagne", "sport"],
        "accessibleHandicap": False
    },
    {
        "nom": "5 √©toiles √† Chamonix option fondue",
        "labels": ["montagne", "detente"],
        "accessibleHandicap": True
    },
    {
        "nom": "Palavas de paillotes en paillotes",
        "labels": ["plage", "ville", "detente"],
        "accessibleHandicap": True
    },
    {
        "nom": "5 √©toiles en rase campagne",
        "labels": ["campagne", "detente"],
        "accessibleHandicap": True
    },
    {
        "nom": "Randonn√©e camping en Loz√®re",
        "labels": ["sport", "montagne", "campagne"],
        "accessibleHandicap": False
    }
]


# =============================
#   PROMPTS
# =============================

PROMPT_EXTRACTION = """Tu es un extracteur de crit√®res de voyage.

CRIT√àRES (6 cl√©s JSON obligatoires) :
- plage, montagne, ville, sport, detente, acces_handicap

R√àGLES D'EXTRACTION :
- true  = avis POSITIF explicite (ex: "je veux", "j'aime", "avec")
- false = avis N√âGATIF explicite (ex: "pas de", "sans", "√©viter")
- null  = crit√®re NON mentionn√© dans le message

EXEMPLES :
- "sport en montagne" ‚Üí sport=true, montagne=true, autres=null
- "plage sans sport" ‚Üí plage=true, sport=false, autres=null
- "d√©tente" ‚Üí detente=true, autres=null

Message utilisateur :
"{message}"

R√©ponds UNIQUEMENT avec un JSON valide au format Criteres."""

PROMPT_CLARIFICATION = """Je n'ai pas identifi√© de crit√®res clairs dans votre message.

Pouvez-vous pr√©ciser vos pr√©f√©rences parmi :
- Plage
- Montagne
- Ville
- Sport
- D√©tente
- Accessibilit√© PMR

Exemple : "Je cherche un s√©jour √† la plage avec d√©tente" """

PROMPT_GENERATION = """Tu es conseiller voyage. Pr√©sente CE voyage uniquement :

MESSAGE UTILISATEUR : {message}
CRIT√àRES IDENTIFI√âS : {criteres}
VOYAGE : {nom}
LABELS : {labels}
ACCESSIBLE HANDICAP : {accessible}

R√©dige 3-4 phrases courtes :
- Reformule la demande (crit√®res actifs seulement)
- Pr√©sente le voyage (nom + points forts pertinents)
- Termine par : "Souhaitez-vous pr√©ciser pour d'autres id√©es ?"

AUCUN emoji. Ton professionnel."""

PROMPT_AUCUN_MATCH = """Aucun voyage ne correspond exactement √† vos crit√®res.

Pouvez-vous ajuster vos pr√©f√©rences ?
Par exemple :
- Retirer un crit√®re contraignant
- Ajouter plus de d√©tails
- Choisir entre plusieurs options"""


# =============================
#   FONCTIONS UTILITAIRES
# =============================

@traceable(name="match_criteres")
def match_criteres(voyage: Dict, criteres: Dict) -> bool:
    """V√©rifie si un voyage correspond aux crit√®res (logique simple)
    
    Trac√© dans LangSmith pour analyser la logique de matching
    """
    for critere, valeur in criteres.items():
        if valeur is None:
            continue
        
        # Cas sp√©cial : accessibilit√© handicap
        if critere == "acces_handicap":
            if valeur != voyage.get("accessibleHandicap", False):
                return False
        else:
            # Mapping crit√®re ‚Üí labels attendus
            label_map = {
                "plage": ["plage"],
                "montagne": ["montagne"],
                "ville": ["ville"],
                "sport": ["sport"],
                "detente": ["detente", "d√©tente"]
            }
            labels_requis = label_map.get(critere, [])
            has_label = any(l in voyage["labels"] for l in labels_requis)
            
            # Si crit√®re = True, le voyage doit avoir le label
            if valeur and not has_label:
                return False
            # Si crit√®re = False, le voyage ne doit PAS avoir le label
            if not valeur and has_label:
                return False
    
    return True


@traceable(name="trouver_voyage")
def trouver_voyage(criteres: Dict) -> Optional[Dict]:
    """Retourne le voyage correspondant le mieux aux crit√®res
    
    Trac√© dans LangSmith pour analyser le processus de s√©lection
    """
    # Trouver tous les voyages compatibles
    matches = []
    for voyage in VOYAGES:
        if match_criteres(voyage, criteres):
            matches.append(voyage)
    
    if not matches:
        return None
    
    # Si un seul match, le retourner
    if len(matches) == 1:
        return matches[0]
    
    # Scoring : favoriser pr√©cision et √©viter le "bruit"
    def score_voyage(voyage: Dict) -> tuple:
        criteres_actifs = {k: v for k, v in criteres.items() if v is True}
        
        # Mapping crit√®re ‚Üí label
        label_map = {
            "plage": "plage",
            "montagne": "montagne",
            "ville": "ville",
            "sport": "sport",
            "detente": "detente"
        }
        
        # Compter les correspondances
        matches_count = 0
        for crit in criteres_actifs:
            label = label_map.get(crit)
            if label and label in voyage["labels"]:
                matches_count += 1
        
        # Compter les labels "pertinents" (ceux dans label_map)
        relevant_labels = [l for l in voyage["labels"] if l in label_map.values()]
        total_relevant = len(relevant_labels)
        
        # Bonus accessibilit√© si demand√©e
        acces_bonus = 0
        if criteres.get("acces_handicap") is True and voyage.get("accessibleHandicap"):
            acces_bonus = 1
        
        # Score : (correspondances, -labels_superflus, accessibilit√©)
        # Le "-" inverse pour favoriser MOINS de labels superflus
        return (matches_count, -total_relevant, acces_bonus)
    
    # Retourner le voyage avec le meilleur score (tuple compar√© √©l√©ment par √©l√©ment)
    best = max(matches, key=score_voyage)
    return best


@traceable(name="generer_reponse_llm")
async def generer_reponse_llm(voyage: Dict, criteres: Dict, message: str) -> str:
    """G√©n√®re une r√©ponse naturelle avec le LLM
    
    Trac√© dans LangSmith pour monitorer les appels LLM et r√©ponses
    """
    try:
        model = init_chat_model("mistral-small-latest", model_provider="mistralai")
        
        # Filtrer les crit√®res actifs (non-None)
        criteres_actifs = {k: v for k, v in criteres.items() if v is not None}
        
        prompt = PROMPT_GENERATION.format(
            message=message,
            criteres=criteres_actifs,
            nom=voyage["nom"],
            labels=", ".join(voyage["labels"]),
            accessible="Oui" if voyage["accessibleHandicap"] else "Non"
        )
        
        response = await model.ainvoke(prompt)
        return response.content
    
    except Exception as e:
        # Log de l'erreur pour le d√©bogage
        print(f"‚ùå Erreur lors de la g√©n√©ration de r√©ponse: {type(e).__name__}: {str(e)}")
        
        # Retourner une r√©ponse de secours user-friendly
        return f"""Je vous recommande : {voyage['nom']}

Ce voyage correspond √† vos crit√®res. Malheureusement, je rencontre un probl√®me technique pour g√©n√©rer une description d√©taill√©e.

Caract√©ristiques :
- Type: {', '.join(voyage['labels'])}
- Accessibilit√© PMR: {'Oui' if voyage['accessibleHandicap'] else 'Non'}

Souhaitez-vous plus d'informations ou explorer d'autres options ?"""


# =============================
#   N≈íUD UNIQUE (conforme examen)
# =============================

@traceable(
    name="process_message",
    metadata={
        "node_type": "main_processor",
        "description": "N≈ìud unique - Extraction + Matching + G√©n√©ration"
    }
)
async def process_message(state: State) -> Dict[str, Any]:
    """
    N≈ìud unique - Cycle complet conforme examen :
    1. Extraction structured output
    2. RESET crit√®res (obligatoire)
    3. Application nouveaux crit√®res
    4. Validation : all(None) ?
    5. Matching + g√©n√©ration r√©ponse
    
    Enti√®rement trac√© dans LangSmith pour analyse compl√®te
    """
    message = state.dernier_message_utilisateur
    
    # 1. EXTRACTION avec structured output
    try:
        model = init_chat_model("mistral-small-latest", model_provider="mistralai")
        model_struct = model.with_structured_output(Criteres)
        
        prompt_extraction = PROMPT_EXTRACTION.format(message=message)
        extraits = await model_struct.ainvoke(prompt_extraction)
        
        # Log des crit√®res extraits (visible dans LangSmith)
        print(f"üìä Crit√®res extraits: {extraits.dict()}")
    
    except Exception as e:
        # Log de l'erreur pour le d√©bogage
        print(f"‚ùå Erreur lors de l'extraction des crit√®res: {type(e).__name__}: {str(e)}")
        
        # En cas d'erreur d'extraction, retourner un message d'erreur user-friendly
        message_erreur = """Je rencontre un probl√®me technique pour analyser votre demande.

Pourriez-vous reformuler votre demande en pr√©cisant vos pr√©f√©rences parmi :
- Plage
- Montagne  
- Ville
- Sport
- D√©tente
- Accessibilit√© PMR

Exemple : "Je cherche un s√©jour √† la plage avec d√©tente" """
        
        return {
            "dernier_message_ia": message_erreur,
            "criteres": {k: None for k in state.criteres}
        }
    
    # 2. RESET OBLIGATOIRE des crit√®res (pas d'h√©ritage entre tours)
    nouveaux_criteres = {k: None for k in state.criteres}
    
    # 3. APPLICATION des nouveaux crit√®res extraits
    for k, v in extraits.dict().items():
        if v is not None:
            nouveaux_criteres[k] = v
    
    # 4. VALIDATION : aucun crit√®re rempli ?
    if all(v is None for v in nouveaux_criteres.values()):
        print("‚ö†Ô∏è  Aucun crit√®re identifi√© - Demande de clarification")
        return {
            "dernier_message_ia": PROMPT_CLARIFICATION,
            "criteres": nouveaux_criteres
        }
    
    # 5. MATCHING : recherche du 1er voyage correspondant
    voyage = trouver_voyage(nouveaux_criteres)
    
    if voyage:
        print(f"‚úÖ Voyage trouv√©: {voyage['nom']}")
        # G√©n√©ration r√©ponse avec LLM
        message_ia = await generer_reponse_llm(voyage, nouveaux_criteres, message)
    else:
        print("‚ùå Aucun voyage correspondant aux crit√®res")
        # Aucun voyage ne correspond
        message_ia = PROMPT_AUCUN_MATCH
    
    return {
        "dernier_message_ia": message_ia,
        "criteres": nouveaux_criteres
    }


# =============================
#   CONSTRUCTION DU GRAPHE
# =============================

def build_graph():
    """Construit le graphe LangGraph minimaliste
    
    Le graphe sera automatiquement trac√© dans LangSmith via langgraph dev
    
    Note: Pas de checkpointer d√©fini ici car langgraph dev g√®re 
    automatiquement la persistance via sa plateforme.
    """
    workflow = StateGraph(State)
    
    # 1 seul n≈ìud
    workflow.add_node("process_message", process_message)
    
    # Ar√™tes simples (pas de conditionnelles)
    workflow.set_entry_point("process_message")
    workflow.add_edge("process_message", END)
    
    # Compilation SANS checkpointer - g√©r√© automatiquement par langgraph dev
    graph = workflow.compile(name="Agent Voyage Examen")
    
    if LANGSMITH_ENABLED:
        print("üîç Graphe compil√© - Tra√ßage actif dans LangSmith")
        print("üíæ Persistance g√©r√©e automatiquement par LangGraph API")
    
    return graph


# Export pour langgraph dev
graph = build_graph()
